{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mdg3KtKw7sDc"
      },
      "outputs": [],
      "source": [
        "import numpy as np # to handle matrix and data operation\n",
        "import pandas as pd # to read csv and handle dataframe\n",
        "from scipy import signal\n",
        "from scipy import misc\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import Compose\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.nn.modules import Module\n",
        "from torch.nn.modules.utils import _single, _pair, _triple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y0OKixS72DZ"
      },
      "source": [
        "Import dataset (CIFAR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "e0e1eecc93184f068aaa41395f6f7736",
            "30342956d7ca4540b106772ae8aa3edc",
            "1a553a1a42644fcb9cdd2dd7e0e5e3c1",
            "f29fa20e0c8d4223990790082110897f",
            "502bcb3831834409b84b09eba716a9ac",
            "38caad8368174fbf939f363e2c53264d",
            "66bc72dc372b4816afae1d3040f841b6",
            "ced6d91ab86c4cf380d9ee7a030d4b88",
            "5315dbd5edc54ba1809a731a64566af8",
            "bec81be1560c4f6588f1644dfc18e661",
            "37b4e3bfad78408aab885017c250a6aa"
          ]
        },
        "id": "XTCKeZr-7yDi",
        "outputId": "168ff6e6-8758-481b-b4df-fa1991bc85be"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "train_transform = Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "batch_size = 32\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform, target_transform=None, download=True)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,drop_last=True,\n",
        "                                          shuffle=True, pin_memory=False, num_workers=8)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=test_transform, target_transform=None, download=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,drop_last=True,\n",
        "                                         shuffle=False, pin_memory=False, num_workers=8)\n",
        "                                         \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcvDoyHi8E5d"
      },
      "source": [
        "Define the network structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3lg2K4Q95qV",
        "outputId": "1cf22ca4-0097-451a-87cb-0c5f3028bfe0"
      },
      "outputs": [],
      "source": [
        "#Check and print the GPU information\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"GPU not detected! Please enable GPU for faster training!\")\n",
        "device = torch.device(\"cuda:0\")\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hvD1VXi8I-D3"
      },
      "outputs": [],
      "source": [
        "#Function to evaluate the inference accuracy\n",
        "def test():\n",
        "  net.eval()\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in testloader:\n",
        "          images, labels = data\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "          outputs = net(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network on the 10000 test images: %6.2f %%' % (\n",
        "      100 * correct / total))\n",
        "  return (100 * correct / total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOebmqXb79md",
        "outputId": "bc7f0287-c62d-4c6a-b1d5-aecb83b18525"
      },
      "outputs": [],
      "source": [
        "class _ConvNd(Module):\n",
        "\n",
        "    __constants__ = ['stride', 'padding', 'dilation', 'groups', 'bias',\n",
        "                     'padding_mode', 'output_padding', 'in_channels',\n",
        "                     'out_channels', 'kernel_size']   \n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, batch_size, stride,\n",
        "                 padding, dilation, transposed, output_padding,\n",
        "                 groups, bias, padding_mode):\n",
        "        super(_ConvNd, self).__init__()\n",
        "        if in_channels % groups != 0:\n",
        "            raise ValueError('in_channels must be divisible by groups')\n",
        "        if out_channels % groups != 0:\n",
        "            raise ValueError('out_channels must be divisible by groups')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_size = batch_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.transposed = transposed\n",
        "        self.output_padding = output_padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "        if transposed:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                in_channels, out_channels // groups, *kernel_size))\n",
        "        else:\n",
        "            self.weight = Parameter(torch.Tensor(\n",
        "                out_channels, in_channels // groups, *kernel_size))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_channels))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        s = ('{in_channels}, {out_channels}, kernel_size={kernel_size}'\n",
        "             ', stride={stride}')\n",
        "        if self.padding != (0,) * len(self.padding):\n",
        "            s += ', padding={padding}'\n",
        "        if self.dilation != (1,) * len(self.dilation):\n",
        "            s += ', dilation={dilation}'\n",
        "        if self.output_padding != (0,) * len(self.output_padding):\n",
        "            s += ', output_padding={output_padding}'\n",
        "        if self.groups != 1:\n",
        "            s += ', groups={groups}'\n",
        "        if self.bias is None:\n",
        "            s += ', bias=False'\n",
        "        return s.format(**self.__dict__)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(_ConvNd, self).__setstate__(state)\n",
        "        if not hasattr(self, 'padding_mode'):\n",
        "            self.padding_mode = 'zeros'\n",
        "\n",
        "class FTconvlayer(_ConvNd):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, batch_size = 16, stride=1,\n",
        "                 padding=0, dilation=1, groups=1,\n",
        "                 bias=True, padding_mode='zeros'):\n",
        "        batch_size = 16\n",
        "        kernel_size = _pair(kernel_size) \n",
        "        stride = _pair(stride)\n",
        "        padding = _pair(padding)\n",
        "        dilation = _pair(dilation)\n",
        "        super(FTconvlayer, self).__init__(\n",
        "            in_channels, out_channels, kernel_size, batch_size, stride, padding, dilation,\n",
        "            False, _pair(0), groups, bias, padding_mode)\n",
        "\n",
        "    def quantization_n(self, input, n = 1, max = 1):\n",
        "      intv = max/(2**n-1)\n",
        "      qunt = torch.ceil(torch.mul(input,(1/intv)))  \n",
        "      out = torch.mul(qunt,intv)\n",
        "      out = torch.clamp(out, min=0, max=max) #make sure the quantized version lies in the interval 0-1, if it's bigger than one just clamp it at one\n",
        "      return(out)  \n",
        "\n",
        "    def input_quant(self, input, level = 5):\n",
        "        max = torch.max(input)\n",
        "        intv = max/level\n",
        "        qunt = torch.floor(torch.mul(input,(1/intv)))\n",
        "        out = torch.mul(qunt, intv)\n",
        "        out = torch.clamp(out, min=0, max=max)\n",
        "        return(out)\n",
        "    \n",
        "    def weightclamp(self, input):\n",
        "      return input.clamp_(0)\n",
        "\n",
        "    def make_complex(self, x):  #converts a real tensor into complex form by adding one extra dimension to it\n",
        "        x_i = torch.cuda.FloatTensor(x.shape).fill_(0)\n",
        "        y = torch.stack((x,x_i),-1)\n",
        "        return torch.view_as_complex(y)\n",
        "\n",
        "    def neg_complex_exp(self, x):    #since pytorch does not support complex exponential, implemented using euler formula exp(-jx)=cos(-x)+jsin(-x)\n",
        "        x_cos = torch.cos(-x)\n",
        "        x_sin = torch.sin(-x)\n",
        "        x_euler = torch.stack((x_cos, x_sin), -1)\n",
        "        return torch.view_as_complex(x_euler)\n",
        "\n",
        "    def complex_mul(self, x,y):  #this implementation should support broadcasting \n",
        "        result = x*y\n",
        "        return result\n",
        "\n",
        "    def conj_transpose(self, x):  #should support broadcasting\n",
        "        x = torch.view_as_real(x)\n",
        "        size = len(x.size())\n",
        "        x_r = x[...,0]\n",
        "        x_i = x[...,1]\n",
        "        x_i_c =-x_i\n",
        "        x_conj = torch.stack((x_r,x_i_c),-1)\n",
        "        x_conj_t = torch.transpose(x_conj, size-3, size-2)#size-1 is the dimension for complex representation\n",
        "        return torch.view_as_complex(x_conj_t)\n",
        "\n",
        "    def roll_n(self, X, axis, n):\n",
        "        f_idx = tuple(slice(None, None, None) if i != axis else slice(0, n, None) for i in range(X.dim()))\n",
        "        b_idx = tuple(slice(None, None, None) if i != axis else slice(n, None, None) for i in range(X.dim()))\n",
        "        front = X[f_idx]\n",
        "        back = X[b_idx]\n",
        "        return torch.cat([back, front], axis)   \n",
        "\n",
        "    def batch_fftshift2d(self, x):\n",
        "        real, imag = torch.unbind(x, -1)\n",
        "        for dim in range(len(real.size())-2, len(real.size())):\n",
        "            n_shift = real.size(dim)//2\n",
        "            if real.size(dim) % 2 != 0:\n",
        "                n_shift += 1  # for odd-sized images\n",
        "            real = self.roll_n(real, axis=dim, n=n_shift)\n",
        "            imag = self.roll_n(imag, axis=dim, n=n_shift)\n",
        "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
        "\n",
        "\n",
        "    def batch_ifftshift2d(self, x):\n",
        "        real, imag = torch.unbind(x, -1)\n",
        "        for dim in range(len(real.size()) - 1, len(real.size())-3, -1):\n",
        "            real = self.roll_n(real, axis=dim, n=real.size(dim)//2)\n",
        "            imag = self.roll_n(imag, axis=dim, n=imag.size(dim)//2)\n",
        "        return torch.stack((real, imag), -1)  # last dim=2 (real&imag)\n",
        "\n",
        "    def propTF(self, u1,L,lambdaa,z):\n",
        "        batch,M,N = u1.shape\n",
        "        dx = L/M\n",
        "        fx = torch.arange(-1/(2*dx),1/(2*dx), 1/L).cuda()\n",
        "        FX,FY = torch.meshgrid(fx,fx)\n",
        "        H = self.neg_complex_exp(math.pi*lambdaa*z*(FX**2+FY**2))\n",
        "        H = torch.fft.fftshift(H)\n",
        "        U1 = torch.fft.fft2(torch.fft.fftshift(u1)) \n",
        "        U2 = self.complex_mul(H,U1)\n",
        "        u2 = torch.fft.ifftshift(torch.fft.ifft2(U2)) #\n",
        "        return u2\n",
        "\n",
        "\n",
        "    def seidel_5(self, u0, v0, X, Y, wd, w040, w131, w222, w220, w311):\n",
        "        beta = math.atan2(u0,v0)\n",
        "        u0r=math.sqrt(u0**2+v0**2)\n",
        "        Xr=X*math.cos(beta)+Y*math.sin(beta)\n",
        "        Yr=-X*math.sin(beta)+Y*math.cos(beta)\n",
        "        rho2=Xr**2+Yr**2\n",
        "        w=wd*rho2+w040*rho2**2+w131*u0r*rho2*Xr+ w222*u0r**2*Xr**2+w220*u0r**2*rho2+w311*math.pow(u0r,3)*Xr\n",
        "        return w\n",
        "\n",
        "    def circ(self, r):\n",
        "        out = torch.abs(r)<=1\n",
        "        return out\n",
        "\n",
        "    ''' for block mean pytorch does not support reshape using 'F' ordering, so use normal reshape and then permute'''\n",
        "    def blockmean_batch(self, X, V, W):\n",
        "        S=X.shape\n",
        "        B1 = S[0]\n",
        "        B2 = S[1]\n",
        "        M = int(S[2] - S[2]%V)\n",
        "        N = int(S[3] - S[3]%W)\n",
        "        if(M*N == 0):\n",
        "            Y = X\n",
        "            return Y\n",
        "        MV = int(M/V)  \n",
        "        NW = int(N/W)\n",
        "        XM = X[:,:,0:M, 0:N].permute(0,1,3,2).reshape([B1,B2,NW, W, MV, V]).permute(0,1,5,4,3,2)\n",
        "        Y = torch.sum(torch.sum(XM,2),3) * (1/(V*W))\n",
        "        return Y\n",
        "\n",
        "    def extract_result(self,input,img_size):\n",
        "        size = input.shape[-1]\n",
        "        start = int((size-4*img_size)/2)\n",
        "        end = start + 4*img_size\n",
        "        output = input[:,:,start:end,start:end]\n",
        "        return output\n",
        "    \n",
        "    def input_pad(self,input,padsize):\n",
        "      input_size = input.shape[2]\n",
        "      pad_size_x = int((padsize-input_size)/2)\n",
        "      pad_size_y = int((padsize-input_size)/2)\n",
        "      p2d = (pad_size_x, pad_size_y, pad_size_x, pad_size_y)\n",
        "      input_pad = F.pad(input, p2d, \"constant\", 0)\n",
        "      return input_pad \n",
        "\n",
        "    def input_adjust(self, input):\n",
        "      input = torch.mul(input,5)\n",
        "      input = torch.floor(input)\n",
        "      output = torch.clamp(input,min=0,max=1)\n",
        "      return(output)\n",
        "\n",
        "    def evenkernel(self, input):\n",
        "      uptri = torch.triu(input,diagonal = 1)\n",
        "      downtri = torch.flip(torch.triu(input,diagonal = 1),[1,2])\n",
        "      result = uptri+downtri\n",
        "      return result\n",
        "\n",
        "    def extract_result(self,input,img_size):\n",
        "      size = input.shape[-1]\n",
        "      start = int((size-img_size)/2)\n",
        "      end = start + img_size\n",
        "      output = input[:,:,start:end,start:end]\n",
        "      return output\n",
        "    \n",
        "    def norm(self,input):\n",
        "      size = input.shape\n",
        "      output = torch.cuda.FloatTensor(size).fill_(0)\n",
        "      for i in range(size[0]):\n",
        "        for j in range(size[1]):\n",
        "          orig = input[i,j,:,:]\n",
        "          maxi = torch.max(orig)\n",
        "          mini = torch.min(orig)\n",
        "          output[i,j,:,:] = (orig-mini)/(maxi-mini)\n",
        "      return output\n",
        "\n",
        "    def accurate_model_forward(self, input, weight):\n",
        "        err = 1e-8\n",
        "        with torch.no_grad():\n",
        "          input = self.input_quant(input)\n",
        "        xx = 208\n",
        "        yy = xx \n",
        "        w = 32    \n",
        "        n_filter_actual = int(self.out_channels/2)\n",
        "        output_full = torch.cuda.FloatTensor(16,self.out_channels,w,w).fill_(0)\n",
        "        output_sub = torch.cuda.FloatTensor(16, int(self.out_channels/2), w, w).fill_(0)\n",
        "        idledmd = torch.cuda.FloatTensor(208, 208).fill_(1)\n",
        "        M,N = idledmd.shape\n",
        "        L1=1.90e-2*xx/208\n",
        "        L2=1.09e-2*yy/208\n",
        "        du=L1/M\n",
        "        dv=L2/N\n",
        "        lambdaa = 0.633e-6\n",
        "        k=2*math.pi/lambdaa\n",
        "        \n",
        "        '''Lens Diffraction (Aperture) and Aberration'''\n",
        "        fu = torch.arange(-1/(2*du),1/(2*du),1/L1)\n",
        "        fv = torch.arange(-1/(2*dv),1/(2*dv),1/L2)\n",
        "        Dxp = 5e-2\n",
        "        wxp = Dxp/2\n",
        "        zxp = 200e-3\n",
        "        lz = lambdaa*zxp\n",
        "        u0 = 0\n",
        "        v0 = 0\n",
        "        f0 = wxp/(lambdaa*zxp)\n",
        "        '''Lens parameter for aberration (Seidel coefficients), wavefront alteration from spherical waves'''\n",
        "        wd=0*lambdaa\n",
        "        w040=4.963*lambdaa\n",
        "        w131=2.637*lambdaa\n",
        "        w222=9.025*lambdaa\n",
        "        w220=7.536*2*lambdaa\n",
        "        w311=0.157*12*lambdaa\n",
        "        \n",
        "        Fu,Fv = torch.meshgrid(fu,fv)\n",
        "        Fu = torch.transpose(Fu,0,1)\n",
        "        Fv = torch.transpose(Fv,0,1)\n",
        "        W = self.seidel_5(u0,v0,-lz*Fu/wxp,-lz*Fv/wxp,wd,w040,w131,w222,w220,w311).cuda() #same as the matlab calculation\n",
        "        #H = circ(torch.sqrt(Fu**2 + Fv**2)/f0)*torch.exp(-1j*k*W)#same as matlab calculation\n",
        "        H = self.complex_mul(self.make_complex(self.circ(torch.sqrt(Fu**2 + Fv**2)/f0).float().cuda()),self.neg_complex_exp(k*W))\n",
        "#-----------------------from here is the actual training, before the loop is basically constants/parameters genreation, which does not needs to be backproped     \n",
        "        for c_in in range(input.shape[1]): # iters for number of input channels\n",
        "            signal = input[:,c_in,:,:] #the dimension of signal is 3, with one batch dimension\n",
        "            weight_raw = weight[:,c_in,:,:] #the dimension of weights are now 3\n",
        "            weight_raw.data = self.quantization_n(weight_raw.data, 1, 1)\n",
        "            dmd_1 = self.make_complex(self.input_pad(signal,208)) #dimension of dmd1 is now 4, first dimension is now batch dimension, so propTF needs to be changed accordingly\n",
        "            \n",
        "            #now need to implement propTF function\n",
        "            u2 = self.propTF(dmd_1,1.9e-2, lambdaa, 1.9e-2)\n",
        "            \n",
        "            '''Fourier Transform after first lens'''\n",
        "            Gg = torch.fft.fftshift(torch.fft.fft2(u2))\n",
        "            Gi = self.complex_mul(Gg,self.conj_transpose(H))\n",
        "            Gi = Gi.unsqueeze(1)\n",
        "            \n",
        "            '''dot product in the fourier plane'''\n",
        "            Gii = self.complex_mul(Gi,self.make_complex(weight_raw))\n",
        "            '''Then get the result in real space'''\n",
        "            Grs = torch.fft.ifft2(torch.fft.ifftshift(Gii))\n",
        "            Grs = torch.view_as_real(Grs)\n",
        "            Ii = torch.sqrt(Grs[...,0]**2+Grs[...,1]**2+err)\n",
        "            op_abs = self.extract_result(Ii,32)\n",
        "            output_full += op_abs\n",
        "        return output_full\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.accurate_model_forward(input, self.weight)\n",
        "\n",
        "\n",
        "class FFTconv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FFTconv, self).__init__()\n",
        "        self.conv1 = FTconvlayer(3, 16, 208) #outdimension should be [32,16,28,28] (1/3,:,5/28/32) depends on whether training in fourier domain or not\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.fc01 = nn.Linear(16 * 16 * 16, 256)\n",
        "        self.fc02 = nn.Linear(256, 10)\n",
        "        self.drop_layer = nn.Dropout(p=0.3)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.pool1(self.conv1(x)))\n",
        "        x = x.view(-1, 16 * 16 * 16)\n",
        "        x = F.relu(self.fc01(x))\n",
        "        x = self.fc02(x)\n",
        "        return x\n",
        "\n",
        "net = FFTconv()    \n",
        "\n",
        "\n",
        "if device:\n",
        "    net.to(device)\n",
        "    print(\"put net onto GPU\")\n",
        "print(net)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RamA6QAh9sot",
        "outputId": "da793c1b-5429-4588-a250-d1b85cd5d9a2"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "n_epoch = 15\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001) #default learning rate for adam is 0.001\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, init_lr, freq):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every n epochs\"\"\"\n",
        "    lr = init_lr * (0.5 ** (epoch // freq))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "best_acc = 0\n",
        "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
        "    print('Training epoch ...')\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    adjust_learning_rate(optimizer, epoch, 0.001, 8)\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:    # print every 200 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 200))\n",
        "            running_loss = 0.0\n",
        "    scheduler.step()\n",
        "    inf_acc = test()\n",
        "    best_acc = max(best_acc, inf_acc)\n",
        "\n",
        "print('Training finished, best accuracy is ', best_acc)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Gen1_5 paper response.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1a553a1a42644fcb9cdd2dd7e0e5e3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ced6d91ab86c4cf380d9ee7a030d4b88",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5315dbd5edc54ba1809a731a64566af8",
            "value": 170498071
          }
        },
        "30342956d7ca4540b106772ae8aa3edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38caad8368174fbf939f363e2c53264d",
            "placeholder": "​",
            "style": "IPY_MODEL_66bc72dc372b4816afae1d3040f841b6",
            "value": ""
          }
        },
        "37b4e3bfad78408aab885017c250a6aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38caad8368174fbf939f363e2c53264d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "502bcb3831834409b84b09eba716a9ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5315dbd5edc54ba1809a731a64566af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66bc72dc372b4816afae1d3040f841b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bec81be1560c4f6588f1644dfc18e661": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced6d91ab86c4cf380d9ee7a030d4b88": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e1eecc93184f068aaa41395f6f7736": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30342956d7ca4540b106772ae8aa3edc",
              "IPY_MODEL_1a553a1a42644fcb9cdd2dd7e0e5e3c1",
              "IPY_MODEL_f29fa20e0c8d4223990790082110897f"
            ],
            "layout": "IPY_MODEL_502bcb3831834409b84b09eba716a9ac"
          }
        },
        "f29fa20e0c8d4223990790082110897f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bec81be1560c4f6588f1644dfc18e661",
            "placeholder": "​",
            "style": "IPY_MODEL_37b4e3bfad78408aab885017c250a6aa",
            "value": " 170499072/? [00:03&lt;00:00, 51102128.99it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
